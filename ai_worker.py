# ai_worker.py
from PyQt6.QtCore import QThread, pyqtSignal
from openai import OpenAI
import json

class AIWorker(QThread):
    reasoning_signal = pyqtSignal(str)
    content_signal = pyqtSignal(str)
    finished_signal = pyqtSignal()
    error_signal = pyqtSignal(str)

    def __init__(self, api_key, base_url, model, temperature, max_tokens, system_prompt, user_prompt):
        super().__init__()
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.system_prompt = system_prompt
        self.user_prompt = user_prompt
        self._is_cancelled = False

    def cancel(self):
        self._is_cancelled = True

    def run(self):
        try:
            client = OpenAI(api_key=self.api_key, base_url=self.base_url)
            response = client.chat.completions.create(
                model=self.model,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": self.user_prompt}
                ],
                stream=True
            )

            for chunk in response:
                if self._is_cancelled:
                    break
                delta = chunk.choices[0].delta
                reasoning = getattr(delta, "reasoning_content", None)
                if reasoning:
                    self.reasoning_signal.emit(reasoning)
                content = getattr(delta, "content", None)
                if content:
                    self.content_signal.emit(content)

            self.finished_signal.emit()
        except Exception as e:
            self.error_signal.emit(str(e))

class AutoPilotWorker(QThread):
    # çŠ¶æ€ä¸UIæ›´æ–°ä¿¡å·
    status_signal = pyqtSignal(str)  # é€šçŸ¥UIå½“å‰åœ¨å¹²å˜›
    log_signal = pyqtSignal(str)  # è¾“å‡ºæ€è€ƒæ—¥å¿—
    content_signal = pyqtSignal(str)  # å®æ—¶æ­£æ–‡è¾“å‡º

    reasoning_signal = pyqtSignal(str)
    start_chapter_signal = pyqtSignal(int, int)  # ä¼ é€’ v_idx, c_idx

    # ç»“æ„æ“ä½œä¿¡å· (è®©ä¸»çº¿ç¨‹å»æ“ä½œæ•°æ®ï¼Œé¿å…è·¨çº¿ç¨‹è¯»å†™å†²çª)
    add_volume_signal = pyqtSignal(str, str)  # vol_name, synopsis
    add_chapter_signal = pyqtSignal(int, str, str)  # v_idx, chap_name, ai_synopsis
    save_content_signal = pyqtSignal(int, int, str, str)  # v_idx, c_idx, content, ai_summary

    # ã€æ–°å¢ã€‘ä¸“é—¨ç”¨äºæ›´æ–°â€œå·²æœ‰ç« èŠ‚â€å’Œâ€œå·²æœ‰å·å®—â€çš„æ¢—æ¦‚
    update_chapter_signal = pyqtSignal(int, int, str)
    update_volume_signal = pyqtSignal(int, str)  # <--- æ–°å¢è¿™è¡Œï¼šä¼ é€’ v_idx, synopsis

    finished_signal = pyqtSignal()
    error_signal = pyqtSignal(str)

    def __init__(self, api_key, base_url, model, temperature, project_meta):
        super().__init__()
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.temperature = temperature
        self.project = project_meta  # ã€ä¿®æ”¹å¤„ã€‘ä¿å­˜æ•´ä¸ª project å¯¹è±¡
        self.meta = project_meta.meta  # ä¼ å…¥å½“å‰é¡¹ç›®çš„å¿«ç…§
        self._is_cancelled = False

    def cancel(self):
        self._is_cancelled = True
        # ã€æ–°å¢ã€‘å¼ºåˆ¶å…³é—­ OpenAI å®¢æˆ·ç«¯ï¼Œæ‰“æ–­å¯èƒ½æ­£åœ¨é˜»å¡çš„ç½‘ç»œè¯·æ±‚
        if hasattr(self, 'client'):
            try:
                self.client.close()
            except Exception:
                pass

    def run(self):
        try:
            self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

            # é˜¶æ®µ 1ï¼šè§„åˆ’åç»­æ‰€æœ‰å·å®—
            self.status_signal.emit("ğŸ”„ é˜¶æ®µ 1/3: æ­£åœ¨ç»Ÿç­¹å…¨å±€ï¼Œè§„åˆ’åç»­å·å®—...")
            self._plan_volumes()
            if self._is_cancelled: return

            # é˜¶æ®µ 2ï¼šéå†å·å®—ï¼Œè§„åˆ’æ¯ä¸€å·çš„è¯¦ç»†ç« èŠ‚
            self.status_signal.emit("ğŸ”„ é˜¶æ®µ 2/3: æ­£åœ¨ä¸ºæ¯ä¸€å·è§„åˆ’ç« èŠ‚ç»†çº²...")
            self._plan_chapters()
            if self._is_cancelled: return

            # é˜¶æ®µ 3ï¼šé€ç« ç”Ÿæˆæ­£æ–‡
            self.status_signal.emit("ğŸ”„ é˜¶æ®µ 3/3: å¼€å¯å…¨è‡ªåŠ¨æŒ‚æœºç å­—æ¨¡å¼ï¼")
            self._generate_all_contents()

            if not self._is_cancelled:
                self.status_signal.emit("âœ… å…¨ä¹¦æŒ‚æœºç”Ÿæˆå®Œæ¯•ï¼")
            self.finished_signal.emit()

        except Exception as e:
            self.error_signal.emit(str(e))

    def _call_llm_for_json(self, system_prompt, user_prompt):
        """è¯·æ±‚ LLM å¹¶å¼ºåˆ¶è¿”å› JSON æ ¼å¼"""
        response = self.client.chat.completions.create(
            model=self.model,
            temperature=self.temperature,
            response_format={"type": "json_object"},  # å¼ºåˆ¶JSONè¾“å‡º
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        return json.loads(response.choices[0].message.content)

    def _plan_volumes(self):
        existing_vols_info = []
        has_blank_volumes = False

        # éå†æ’æŸ¥æœ‰æ²¡æœ‰â€œç©ºå·â€
        for v in self.meta["volumes"]:
            syn = v.get("synopsis", "")
            if len(syn.strip()) < 10:
                has_blank_volumes = True
            existing_vols_info.append({
                "name": v["name"],
                "synopsis": syn
            })

        current_vol_count = len(self.meta["volumes"])

        # ã€ä¿®æ”¹å¤„ã€‘åŒé‡åˆ¤å®šï¼šæ•°é‡è¾¾æ ‡ ä¸” æ²¡æœ‰ç©ºå·ï¼Œæ‰è·³è¿‡
        if current_vol_count >= 5 and not has_blank_volumes:
            self.log_signal.emit("â­ï¸ å½“å‰å·æ•°å·²è¾¾æ ‡ï¼ˆ>=5å·ï¼‰ä¸”æ— ç©ºç™½å·æ¢—æ¦‚ï¼Œè·³è¿‡å·å®—è§„åˆ’ã€‚")
            return

        global_synopsis = self.meta.get("global_synopsis", "")

        sys_prompt = "ä½ æ˜¯ä¸€ä½ç½‘æ–‡å†™æ‰‹ã€‚å¿…é¡»è¿”å›ä¸¥æ ¼çš„JSONå¯¹è±¡ã€‚"
        user_prompt = f"""
ã€å…¨å±€å¤§çº²ã€‘
{global_synopsis}

ã€ç›®å‰å·²æœ‰çš„å·å®—ä¿¡æ¯ã€‘
{json.dumps(existing_vols_info, ensure_ascii=False)}

ä»»åŠ¡æŒ‡ä»¤ï¼š
1. éå†ã€ç›®å‰å·²æœ‰çš„å·å®—ä¿¡æ¯ã€‘ã€‚å¦‚æœæŸå·çš„ synopsis ä¸ºç©ºæˆ–éå¸¸ç®€çŸ­ï¼Œè¯·ä¸¥æ ¼ä¾æ®ã€å…¨å±€å¤§çº²ã€‘å’Œä¸Šä¸‹æ–‡ï¼Œä¸ºå…¶æ‰©å†™ä¸ºè¯¦ç»†çš„å‰§æƒ…èµ°å‘æ¢—æ¦‚ï¼ˆç»ä¸èƒ½æ”¹å˜åŸæœ‰çš„å·åï¼ï¼‰ã€‚å¦‚æœè¯¥å·çš„ synopsis å·²ç»æœ‰å…·ä½“å†…å®¹ï¼Œè¯·åŸæ ·ä¿ç•™ï¼Œä¸è¦åšä»»ä½•åˆ æ”¹ã€‚
2. åˆ¤æ–­æ•…äº‹æ˜¯å¦å®Œç»“ã€‚å¦‚æœæœªå®Œç»“ï¼Œè¯·åœ¨ new_volumes ä¸­ç»§ç»­è§„åˆ’åç»­æ‰€éœ€çš„æ–°å·å®—ï¼ˆå·åä¸è¯¦ç»†æ¢—æ¦‚ï¼‰ã€‚
3. æ‰©å†™çš„å†…å®¹ä¸å¯è¿‡äºä¿—å¥—

è¿”å›æ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
    "updated_existing_volumes": [
        {{"name": "å·²æœ‰å·å", "synopsis": "æ‰©å†™åæˆ–åŸæ ·ä¿ç•™çš„è¯¦ç»†æ¢—æ¦‚"}}
    ],
    "new_volumes": [
        {{"name": "æ–°å·å", "synopsis": "æ–°è§„åˆ’çš„è¯¦ç»†æ¢—æ¦‚"}}
    ]
}}
å¦‚æœå·²å®Œç»“ï¼Œ"new_volumes" ä¼ ç©ºåˆ—è¡¨ã€‚
"""
        result = self._call_llm_for_json(sys_prompt, user_prompt)

        # 1. å…ˆæ›´æ–°é‚£äº›åŸæœ¬æ¢—æ¦‚ä¸ºç©ºçš„å·²æœ‰å·
        for updated_vol in result.get("updated_existing_volumes", []):
            if self._is_cancelled: break
            for v_idx, v in enumerate(self.meta["volumes"]):
                if v["name"] == updated_vol["name"]:
                    # åªæœ‰å½“åŸå…ˆç¡®å®åçŸ­ï¼Œæˆ–è€…æ›´æ–°å†…å®¹æ›´é•¿æ—¶æ‰è¦†ç›–ï¼Œä¿æŠ¤ç”¨æˆ·è‡ªå·±å†™çš„æ–‡æœ¬
                    if len(v.get("synopsis", "")) < len(updated_vol["synopsis"]):
                        self.update_volume_signal.emit(v_idx, updated_vol["synopsis"])
                        v["synopsis"] = updated_vol["synopsis"]
                        self.log_signal.emit(f"ğŸ“ è¡¥å……ç©ºç™½å·å®—æ¢—æ¦‚ï¼š{v['name']}")
                    break

        # 2. å†å¤„ç†å…¨æ–°å¢åŠ çš„å·
        for vol in result.get("new_volumes", []):
            if self._is_cancelled: break

            # é˜²é‡æœºåˆ¶
            existing_names = [v["name"] for v in self.meta["volumes"]]
            if vol["name"] in existing_names:
                continue

            self.add_volume_signal.emit(vol["name"], vol["synopsis"])
            self.log_signal.emit(f"ğŸ“š è‡ªåŠ¨åˆ›å»ºæ–°å·ï¼š{vol['name']}")

    def _plan_chapters(self):
        for v_idx, vol in enumerate(self.meta["volumes"]):
            if self._is_cancelled: break

            existing_chaps = vol.get("chapters", [])
            current_chap_count = len(existing_chaps)
            has_blank_chapters = False
            existing_chaps_info = []

            for c in existing_chaps:
                ai_syn = c.get("ai_synopsis", "")
                user_syn = c.get("synopsis", "")
                if len(ai_syn.strip()) < 10 and len(user_syn.strip()) < 10:
                    has_blank_chapters = True

                existing_chaps_info.append({
                    "name": c["name"],
                    "user_synopsis": user_syn,
                    "ai_synopsis": ai_syn
                })

            if current_chap_count >= 4 and not has_blank_chapters:
                self.log_signal.emit(f"â­ï¸ {vol['name']} ç« èŠ‚æ•°å·²è¾¾æ ‡(>=4)ä¸”æ— ç©ºç™½æ¢—æ¦‚ï¼Œè·³è¿‡ç»†çº²è§„åˆ’ã€‚")
                continue

            # ã€æ–°å¢é€»è¾‘ã€‘ï¼šåœ¨æ¯ä¸€æ¬¡è§„åˆ’å½“å‰å·çš„ç« èŠ‚å‰ï¼Œé‡æ–°è·å–ä¸€éæ•´æœ¬ä¹¦çš„æœ€æ–°å…¨å±€ä¸Šä¸‹æ–‡
            # è¿™æ ·ä¸ä»…èƒ½çœ‹åˆ°ä»¥å‰çš„å·ï¼Œè¿˜èƒ½å®æ—¶çœ‹åˆ°åˆšåˆšï¼ˆåœ¨æœ¬è½®å¾ªç¯ä¸­ï¼‰è¢« AI æ‰©å†™æˆ–æ–°å»ºå‡ºæ¥çš„ç« èŠ‚ï¼
            all_context_str = "ã€å…¨ä¹¦å…¨å±€å·ç« æ¦‚è§ˆï¼ˆåŒ…å«æœ€æ–°å‰§æƒ…åŠ¨æ€ï¼‰ã€‘\n"
            for temp_v in self.meta["volumes"]:
                all_context_str += f"â–¶ {temp_v['name']} (æœ¬å·æ¢—æ¦‚: {temp_v.get('synopsis', 'æ— ')})\n"
                for temp_c in temp_v.get("chapters", []):
                    # ä¼˜å…ˆè¯»å– AI ä¹‹å‰ç”Ÿæˆçš„è¯¦ç»†æ¢—æ¦‚ï¼Œå¦‚æœæ²¡æœ‰åˆ™é™çº§è¯»å–ç”¨æˆ·çš„ç»†çº²
                    temp_ai_syn = temp_c.get("ai_synopsis", "")
                    temp_user_syn = temp_c.get("synopsis", "")
                    display_syn = temp_ai_syn if temp_ai_syn.strip() else (
                        temp_user_syn if temp_user_syn.strip() else "æš‚æ— æ¢—æ¦‚")
                    all_context_str += f"  - {temp_c['name']}: {display_syn}\n"
                all_context_str += "\n"

            sys_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šä¸”æ³¨é‡ä¼ç¬”ä¸é€»è¾‘è¿è´¯çš„é¡¶çº§ç½‘æ–‡å†™æ‰‹å’Œä¸»ç¼–ã€‚å¿…é¡»è¿”å›ä¸¥æ ¼çš„JSONå¯¹è±¡ã€‚"
            user_prompt = f"""
{all_context_str}

ã€å½“å‰ä»»åŠ¡ç›®æ ‡ã€‘ï¼š{vol['name']}
ã€æœ¬å·æ ¸å¿ƒæ¢—æ¦‚ã€‘ï¼š{vol.get('synopsis', 'æ— ')}
ã€æœ¬å·å·²æœ‰ç« èŠ‚ä¿¡æ¯ã€‘ï¼š{json.dumps(existing_chaps_info, ensure_ascii=False)}

ä»»åŠ¡æŒ‡ä»¤ï¼š
1. è¯·å……åˆ†é˜…è¯»ä¸Šæ–¹çš„ã€å…¨ä¹¦å…¨å±€å·ç« æ¦‚è§ˆã€‘ï¼Œåœ¨è¡¥é½ç« èŠ‚åå’Œæ‰©å†™æ¢—æ¦‚æ—¶ï¼Œå¿…é¡»ç»“åˆæ‰€æœ‰å·å®—æ¢—æ¦‚å’Œå·²æœ‰ç« èŠ‚çš„å‰§æƒ…èµ°å‘ï¼Œç¡®ä¿å‰åå‘¼åº”ã€ä¸åƒä¹¦ã€æƒ…èŠ‚ä¸å‰²è£‚ã€‚
2. éå†ã€æœ¬å·å·²æœ‰ç« èŠ‚ä¿¡æ¯ã€‘ã€‚å¦‚æœæŸç« çš„ ai_synopsis ä¸ºç©ºæˆ–è¾ƒçŸ­ï¼Œè¯·ä¸¥æ ¼ä¾æ®ç”¨æˆ·çš„ user_synopsisï¼ˆç»ä¸èƒ½åæ‰æˆ–æ”¹å˜ç”¨æˆ·åŸæ„ï¼ï¼‰å¹¶ç»“åˆå‰åæ–‡å°†å…¶æ‰©å†™ä¸ºåŒ…å«å…·ä½“æƒ…èŠ‚å’Œç»†èŠ‚çš„è¯¦ç»†æ¢—æ¦‚ã€‚
3. åˆ¤æ–­æœ¬å·æ•…äº‹æ˜¯å¦å®Œç»“ã€‚å¦‚æœæœªå®Œç»“ï¼Œè¯·åœ¨ new_chapters ä¸­ç»“åˆå…¨å±€èƒŒæ™¯ç»§ç»­è§„åˆ’åç»­çš„å…¨æ–°ç« èŠ‚åä¸è¯¦ç»†æ¢—æ¦‚ï¼Œç›´è‡³æœ¬å·å‰§æƒ…å®Œç¾é—­ç¯ã€‚

è¿”å›æ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
    "updated_existing_chapters": [
        {{"name": "å·²æœ‰ç« èŠ‚å", "ai_synopsis": "æ‰©å†™åçš„è¯¦ç»†æ¢—æ¦‚"}}
    ],
    "new_chapters": [
        {{"name": "æ–°ç« èŠ‚å", "ai_synopsis": "æ–°è§„åˆ’çš„è¯¦ç»†æ¢—æ¦‚"}}
    ]
}}
"""
            result = self._call_llm_for_json(sys_prompt, user_prompt)

            for updated_chap in result.get("updated_existing_chapters", []):
                if self._is_cancelled: break
                for c_idx, c in enumerate(vol["chapters"]):
                    if c["name"] == updated_chap["name"]:
                        # åªæœ‰å½“åŸå…ˆç¡®å®åçŸ­ï¼Œæˆ–è€…æ›´æ–°å†…å®¹æ›´é•¿æ—¶æ‰æ›´æ–°ï¼Œä¿æŠ¤å¿ƒè¡€
                        if len(c.get("ai_synopsis", "")) < len(updated_chap["ai_synopsis"]):
                            self.update_chapter_signal.emit(v_idx, c_idx, updated_chap["ai_synopsis"])
                            c["ai_synopsis"] = updated_chap["ai_synopsis"]
                        self.log_signal.emit(f"ğŸ“ è¡¥å……ç©ºç™½ç« èŠ‚ç»†çº²ï¼š{vol['name']} - {c['name']}")
                        break

            for chap in result.get("new_chapters", []):
                if self._is_cancelled: break

                # é˜²é‡æœºåˆ¶
                existing_names = [c["name"] for c in vol["chapters"]]
                if chap["name"] in existing_names:
                    self.log_signal.emit(f"âš ï¸ æ‹¦æˆªåˆ° AI é‡å¤ç”Ÿæˆçš„ç« èŠ‚ï¼š{chap['name']}ï¼Œå·²è‡ªåŠ¨è·³è¿‡ã€‚")
                    continue

                self.add_chapter_signal.emit(v_idx, chap["name"], chap["ai_synopsis"])
                # ã€ä¿®å¤è¯´æ˜ã€‘ï¼šåˆ é™¤äº† vol["chapters"].append ä»£ç ï¼Œå› ä¸ºä¸»çº¿ç¨‹å·²ç»é€šè¿‡ä¿¡å·å¤„ç†äº†
                self.log_signal.emit(f"ğŸ“„ è‡ªåŠ¨è§„åˆ’è¡¥é½æ–°ç« èŠ‚ï¼š{vol['name']} - {chap['name']}")

    def _generate_all_contents(self):
        # éå†æ‰€æœ‰å·å’Œç« ï¼Œå¯»æ‰¾æ²¡æœ‰å†…å®¹ï¼ˆæˆ–è€…è¿˜æ²¡å†™ï¼‰çš„ç« èŠ‚å¼€å§‹å†™
        for v_idx, vol in enumerate(self.meta["volumes"]):
            for c_idx, chap in enumerate(vol["chapters"]):
                if self._is_cancelled: return

                # è¿™é‡Œå‡è®¾å¦‚æœç« èŠ‚è¿˜æ²¡æœ‰å†…å®¹ï¼Œæˆ‘ä»¬å°±è‡ªåŠ¨å†™å®ƒ
                # ä¸ºäº†ç®€ä¾¿ï¼Œæˆ‘ä»¬æ¯æ¬¡ç”Ÿæˆéƒ½ä¼šæŠŠæ­£æ–‡ä¼ å›ä¸»çº¿ç¨‹
                existing_content = self.project.read_chapter_content(vol["name"], chap["name"])
                if len(existing_content.strip()) > 100:
                    self.status_signal.emit(f"â­ï¸ è·³è¿‡å·²å†™ç« èŠ‚ï¼š{vol['name']} - {chap['name']}")
                    continue  # å·²ç»æœ‰å†…å®¹äº†ï¼Œç›´æ¥è·³è¿‡ç”Ÿæˆï¼Œä¿æŠ¤ç”¨æˆ·çš„å¿ƒè¡€ï¼

                self.status_signal.emit(f"âœï¸ æ­£åœ¨æŒ‚æœºç”Ÿæˆï¼š{vol['name']} - {chap['name']}")
                self.log_signal.emit(f"å¼€å§‹æ’°å†™ï¼š{chap['name']}...")

                self.start_chapter_signal.emit(v_idx, c_idx)

                # æ„å»º prompt (ä½¿ç”¨ä¸ä½ ä¹‹å‰ç±»ä¼¼çš„æ–¹æ³•ï¼Œä½†åœ¨ Worker å†…ç»„è£…)
                prev_v_idx, prev_c_idx = -1, -1

                history_str = ""  # ç»„è£…è¿‡å¾€ ai_synopsis
                for i in range(v_idx + 1):
                    v = self.meta["volumes"][i]
                    limit = c_idx if i == v_idx else len(v["chapters"])
                    for j in range(limit):
                        history_str += f" - {v['chapters'][j]['name']}: {v['chapters'][j].get('ai_synopsis', '')}\n"

                        # ã€æ–°å¢ã€‘å¯»æ‰¾å¹¶è¯»å–ä¸Šä¸€ç« çš„æ­£æ–‡å†…å®¹
                        prev_v_idx, prev_c_idx = -1, -1
                        if c_idx > 0:
                            prev_v_idx, prev_c_idx = v_idx, c_idx - 1
                        elif v_idx > 0:
                            # å»ä¸Šä¸€å·æ‰¾æœ€åä¸€ç« 
                            for i in range(v_idx - 1, -1, -1):
                                if len(self.meta["volumes"][i]["chapters"]) > 0:
                                    prev_v_idx = i
                                    prev_c_idx = len(self.meta["volumes"][i]["chapters"]) - 1
                                    break

                prev_chapter_content = ""
                if prev_v_idx != -1 and prev_c_idx != -1:
                    pv_name = self.meta["volumes"][prev_v_idx]["name"]
                    pc_name = self.meta["volumes"][prev_v_idx]["chapters"][prev_c_idx]["name"]
                    # ç›´æ¥è°ƒç”¨ project çš„è¯»å–æ–¹æ³•
                    prev_chapter_content = self.project.read_chapter_content(pv_name, pc_name)
                    # æˆªæ–­å¤ªé•¿çš„ä¸Šä¸€ç« å†…å®¹ (ä¿ç•™å1500å­—å·¦å³å³å¯ï¼ŒèŠ‚çœTokenå¹¶ä¿è¯æ‰¿æ¥)
                    # ã€ä¿®å¤1ã€‘ç¼©çŸ­ä¸Šä¸€ç« ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢æ³¨æ„åŠ›åŠ«æŒ (æ”¹ä¸º1500å­—)
                    if len(prev_chapter_content) > 1500:
                        prev_chapter_content = "...(å‰æ–‡çœç•¥)...\n" + prev_chapter_content[-1500:]

                    # ã€ä¿®å¤2ã€‘æå–ç¼ºå¤±çš„äººç‰©è®¾å®š
                char_texts = [f"ã€{c['name']}ã€‘ æ€§åˆ«:{c['gender']} æ€§æ ¼:{c['personality']} ç»å†:{c['experience']}" for c
                              in self.meta.get("characters", [])]
                char_setting = "\n".join(char_texts) if char_texts else "æœªæä¾›æ˜ç¡®äººç‰©ã€‚"

                sys_prompt = f"""ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ç½‘æ–‡å¤§ç¥ä½œå®¶ã€‚
                    ã€å…¨å±€å¤§çº²ã€‘ï¼š{self.meta.get('global_synopsis', '')}
                    ã€æ ¸å¿ƒäººç‰©è®¾å®šã€‘ï¼š\n{char_setting}
                    ã€è¦æ±‚ã€‘ï¼šç›´æ¥è¾“å‡ºæ­£æ–‡ï¼Œç¦æ­¢ä»»ä½•å¤šä½™çš„å¯’æš„ã€‚åœ¨æ­£æ–‡è¾“å‡ºå®Œæ¯•åï¼Œå¿…é¡»å¦èµ·ä¸€è¡Œå¹¶ä¸¥æ ¼ä»¥ `[AI_SUMMARY]` ä½œä¸ºåˆ†å‰²ç¬¦ï¼Œç„¶åè¾“å‡ºçº¦500å­—é«˜åº¦ç»“æ„åŒ–çš„ã€æœ¬ç« å¤ç›˜ä¸è®°å¿†é”šç‚¹ã€‘ã€‚
åœ¨ `[AI_SUMMARY]` ä¹‹åï¼Œå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹3ä¸ªç»´åº¦è¾“å‡ºï¼ˆå®¢è§‚ã€ç²¾ç‚¼ï¼Œçº¯ä½œå†…éƒ¨è®°å¿†ä½¿ç”¨ï¼‰ï¼š
                    1. æ ¸å¿ƒå‰§æƒ…è„‰ç»œï¼šæŒ‰æ—¶é—´é¡ºåºç®€è¿°æœ¬ç« å‘ç”Ÿçš„å®è´¨æ€§äº‹ä»¶ï¼ˆèµ·å› ã€ç»è¿‡ã€ç»“æœï¼‰ã€‚
                    2. äººç‰©çŠ¶æ€æ›´æ–°ï¼šè®°å½•æœ¬ç« ä¸»è§’åŠé…è§’çš„è¡Œä¸ºåŠå¿ƒæ€ã€‚
                    3. ç‰©å“è®¾å®šæ›´æ–°ï¼šè®°å½•æœ¬ç« æ‰€æœ‰ç‰©å“çŠ¶æ€
"""

                # ã€ä¿®å¤3ã€‘å¼ºåˆ¶ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æ‰‹å†™çš„ synopsis (å¦‚æœä¸ºç©ºæ‰é€€å›ä½¿ç”¨ ai_synopsis)
                user_syn = chap.get("synopsis", "").strip()
                ai_syn = chap.get("ai_synopsis", "").strip()
                target_synopsis = user_syn if user_syn else (ai_syn if ai_syn else "æ— ")

                user_prompt = f"ã€è¿‡å¾€å‰§æƒ…è½¨è¿¹å‚è€ƒã€‘\n{history_str}\n\n"
                if prev_chapter_content.strip():
                    user_prompt += f"ã€ç´§æ¥ä¸Šä¸€ç« çš„æœ«å°¾å†…å®¹ã€‘(å‚è€ƒæ­¤æ®µè¿‡æ¸¡ï¼Œä½†ä¸è¦æ·±é™·å…¶ä¸­)\n{prev_chapter_content.strip()}\n\n"

                # ã€ä¿®å¤4ã€‘åœ¨æœ«å°¾å¼ºè°ƒç”¨å¹å·æå‡â€œæœ¬ç« è¦æ±‚â€çš„æƒé‡
                user_prompt += f"""ã€æœ¬æ¬¡å†™ä½œæ ¸å¿ƒä»»åŠ¡ (æœ€é«˜ä¼˜å…ˆçº§)ã€‘
                å½“å‰æ’°å†™ï¼š{vol['name']} - {chap['name']}
                æœ¬ç« å¿…é¡»å®ç°çš„æƒ…èŠ‚è¦æ±‚ï¼š{target_synopsis}

                ã€è¡ŒåŠ¨æŒ‡ä»¤ã€‘
                è¯·åŠ¡å¿…å°†å‰§æƒ…å‘ã€æœ¬ç« å¿…é¡»å®ç°çš„æƒ…èŠ‚è¦æ±‚ã€‘æ¨è¿›ï¼ä¸è¦è¢«ä¸Šä¸€ç« çš„æœ«å°¾å†…å®¹å›°ä½ï¼Œå¿…é¡»åœ¨æœ¬æ–‡ä¸­è½å®æœ¬ç« è¦æ±‚é‡Œçš„æ‰€æœ‰æ ¸å¿ƒæƒ…èŠ‚å’Œååœºé¢ï¼æ‰©å†™ä¸ºæ–‡ç¬”æµç•…çš„å®Œæ•´æ­£æ–‡ï¼"""

                response = self.client.chat.completions.create(
                    model=self.model,
                    temperature=self.temperature,
                    messages=[
                        {"role": "system", "content": sys_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    stream=True
                )

                content_buffer = ""
                for chunk in response:
                    if self._is_cancelled: break
                    delta = chunk.choices[0].delta
                    # ã€æ–°å¢ã€‘æå–å¹¶å‘é€ AI çš„æ€è€ƒè¿‡ç¨‹
                    reasoning = getattr(delta, "reasoning_content", None)
                    if reasoning:
                        self.reasoning_signal.emit(reasoning)
                    delta_content = getattr(delta, "content", None)
                    if delta_content:
                        content_buffer += delta_content
                        self.content_signal.emit(delta_content)  # å®æ—¶æ¨é€åˆ°ç•Œé¢

                if self._is_cancelled: return

                # æ‹†åˆ†æ­£æ–‡ä¸æ€»ç»“
                parts = content_buffer.split("[AI_SUMMARY]")
                main_content = parts[0].strip()
                ai_summary = parts[1].strip() if len(parts) > 1 else ""

                # å‘Šè¯‰ä¸»çº¿ç¨‹ä¿å­˜æ•°æ®
                self.save_content_signal.emit(v_idx, c_idx, main_content, ai_summary)

class CorrectionWorker(QThread):
    # ä¿¡å·å®šä¹‰
    status_signal = pyqtSignal(str)
    log_signal = pyqtSignal(str)  # ç”¨äºè¾“å‡ºåˆ°å³ä¾§è¾¹æ çš„è®°å½•
    update_text_signal = pyqtSignal(int, int, str, str)  # v_idx, c_idx, new_content, new_summary
    finished_signal = pyqtSignal()
    error_signal = pyqtSignal(str)
    reasoning_signal = pyqtSignal(str)

    def __init__(self, api_key, base_url, model, temperature, project, scope, mode):
        super().__init__()
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.temperature = temperature
        self.project = project
        self.meta = project.meta
        self.scope = scope  # "full" æˆ– "chapter"
        self.mode = mode  # "typo", "setting", "all"
        # ç« èŠ‚çº§åˆ«çº é”™çš„åæ ‡
        self.target_v_idx = -1
        self.target_c_idx = -1
        self._is_cancelled = False

    def set_target(self, v_idx, c_idx):
        self.target_v_idx = v_idx
        self.target_c_idx = c_idx

    def cancel(self):
        self._is_cancelled = True
        if hasattr(self, 'client'):
            try:
                self.client.close()
            except:
                pass

    def run(self):
        try:
            self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

            if self.scope == "chapter":
                self._correct_single_chapter(self.target_v_idx, self.target_c_idx, self.mode)
            elif self.scope == "full":
                self._correct_full_book(self.mode)

            # æ— è®ºæ˜¯å¦è¢«å–æ¶ˆï¼Œæ­£å¸¸é€€å‡ºæ—¶éƒ½å‘ä¸»ç•Œé¢å‘é€ä¿¡å·ï¼Œä»¥æ¢å¤ UI çŠ¶æ€
            self.finished_signal.emit()
        except Exception as e:
            # å¦‚æœæ˜¯æ‰‹åŠ¨å–æ¶ˆå¼•å‘çš„ç½‘ç»œå¼ºè¡Œåˆ‡æ–­å¼‚å¸¸ï¼Œç›´æ¥æ— è§†å¹¶å‘é€ç»“æŸä¿¡å·
            if self._is_cancelled:
                self.finished_signal.emit()
            else:
                self.error_signal.emit(str(e))

    def _call_llm_json(self, sys_prompt, user_prompt):
        resp = self.client.chat.completions.create(
            model=self.model,
            temperature=self.temperature,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": user_prompt}
            ],
            stream=True  # ã€ä¿®æ”¹å¤„ã€‘å¼ºè¡Œå¼€å¯æµå¼ä¼ è¾“ä»¥æˆªè·æ€è€ƒè¿‡ç¨‹
        )

        content_buffer = ""
        for chunk in resp:
            if self._is_cancelled: break
            delta = chunk.choices[0].delta

            # å®æ—¶æå–å¹¶å‘é€æ€è€ƒè¿‡ç¨‹åˆ°ç•Œé¢
            reasoning = getattr(delta, "reasoning_content", None)
            if reasoning:
                self.reasoning_signal.emit(reasoning)

            # ç¼“å†²åå°çš„ JSON æ­£æ–‡
            content = getattr(delta, "content", None)
            if content:
                content_buffer += content

        if self._is_cancelled:
            return {}

        # ç­‰å¾…æµå¼ä¼ è¾“å®Œæ¯•åï¼Œå†ç»Ÿä¸€æŠŠç¼“å†²åŒºé‡Œçš„å­—ç¬¦ä¸²è§£æä¸º JSON
        try:
            return json.loads(content_buffer)
        except json.JSONDecodeError as e:
            self.error_signal.emit(f"AIè¿”å›çš„JSONæ ¼å¼æœ‰è¯¯: {str(e)}")
            return {}

    def _correct_single_chapter(self, v_idx, c_idx, mode):
        vol = self.meta["volumes"][v_idx]
        chap = vol["chapters"][c_idx]
        content = self.project.read_chapter_content(vol["name"], chap["name"])
        ai_summary = chap.get("ai_synopsis", "")

        if not content.strip():
            self.status_signal.emit("âš ï¸ å½“å‰ç« èŠ‚æ— å†…å®¹ï¼Œè·³è¿‡çº é”™ã€‚")
            return

        modes_to_run = ["typo", "setting"] if mode == "all" else [mode]
        current_content = content
        current_summary = ai_summary

        if "setting" in modes_to_run and not self._is_cancelled:
            self.status_signal.emit(f"ğŸ” æ­£åœ¨è¿›è¡Œã€è®¾å®šçº é”™ã€‘: {chap['name']}...")
            current_content, current_summary = self._do_setting_correction(v_idx, c_idx, current_content,
                                                                           current_summary)

        if "typo" in modes_to_run and not self._is_cancelled:
            self.status_signal.emit(f"ğŸ“ æ­£åœ¨è¿›è¡Œã€é”™åˆ«å­—/è¯­ç—…çº é”™ã€‘: {chap['name']}...")
            current_content = self._do_typo_correction(v_idx, c_idx, current_content)

        # ç»Ÿä¸€ä¿å­˜
        self.update_text_signal.emit(v_idx, c_idx, current_content, current_summary)

    def _correct_full_book(self, mode):
        if mode in ["setting", "all"]:
            self.status_signal.emit("ğŸ•µï¸ å¼€å¯å…¨ä¹¦æ‰«ææ¨¡å¼ï¼Œæ­£åœ¨ç»Ÿç­¹å…¨å±€è®¾å®š...")
            # ç¬¬ä¸€é˜¶æ®µï¼šæ’æŸ¥æœ‰é—®é¢˜çš„ç« èŠ‚
            problem_list = self._detect_global_setting_conflicts()
            if self._is_cancelled: return

            if not problem_list:
                self.log_signal.emit("âœ… å…¨ä¹¦è®¾å®šé€»è¾‘ä¸¥å¯†ï¼Œæœªå‘ç°åƒä¹¦æˆ–è®¾å®šçŸ›ç›¾ç°è±¡ï¼")
            else:
                self.log_signal.emit(f"âš ï¸ æ‰«æå®Œæ¯•ï¼Œå‘ç° {len(problem_list)} ä¸ªè®¾å®šçŸ›ç›¾ç« èŠ‚ï¼Œå‡†å¤‡é€ä¸€ä¿®å¤ã€‚")
                # ç¬¬äºŒé˜¶æ®µï¼šéå†ä¿®å¤
                for issue in problem_list:
                    if self._is_cancelled: break
                    v = issue.get("v_idx")
                    c = issue.get("c_idx")
                    reason = issue.get("reason")
                    vol_name = self.meta["volumes"][v]["name"]
                    chap_name = self.meta["volumes"][v]["chapters"][c]["name"]

                    self.status_signal.emit(f"ğŸ”§ æ­£åœ¨ä¿®å¤è®¾å®šçŸ›ç›¾: {vol_name}-{chap_name}...")
                    self.log_signal.emit(f"[{vol_name}-{chap_name}] é”å®šé”™è¯¯: {reason}")

                    old_content = self.project.read_chapter_content(vol_name, chap_name)
                    old_summary = self.meta["volumes"][v]["chapters"][c].get("ai_synopsis", "")
                    new_content, new_summary = self._do_setting_correction(v, c, old_content, old_summary,
                                                                           specific_reason=reason)
                    self.update_text_signal.emit(v, c, new_content, new_summary)

        if mode in ["typo", "all"]:
            self.status_signal.emit("ğŸ“ å¼€å¯å…¨ä¹¦é”™åˆ«å­—/è¯­ç—…æ’æŸ¥...")
            for v_idx, vol in enumerate(self.meta["volumes"]):
                for c_idx, chap in enumerate(vol["chapters"]):
                    if self._is_cancelled: return
                    self.status_signal.emit(f"ğŸ“ æ­£åœ¨æ ¡å¯¹: {vol['name']} - {chap['name']}...")
                    old_content = self.project.read_chapter_content(vol["name"], chap["name"])
                    if old_content.strip():
                        new_content = self._do_typo_correction(v_idx, c_idx, old_content)
                        summary = chap.get("ai_synopsis", "")
                        self.update_text_signal.emit(v_idx, c_idx, new_content, summary)

    def _do_typo_correction(self, v_idx, c_idx, content):
        sys_prompt = "ä½ æ˜¯ä¸€ä¸ªç«çœ¼é‡‘ç›çš„ä¸“ä¸šå°è¯´æ–‡å­—æ ¡å¯¹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ‰¾å‡ºæ­£æ–‡ä¸­çš„é”™åˆ«å­—å’Œè¯­ç—…ï¼Œå¹¶ç›´æ¥ä¿®æ”¹ã€‚å¿…é¡»è¿”å›ä¸¥æ ¼çš„JSONã€‚"
        user_prompt = f"""
è¯·æ ¡å¯¹ä»¥ä¸‹æ­£æ–‡ã€‚
è¦æ±‚ï¼š
1. ä¿®æ­£é”™åˆ«å­—ã€æ ‡ç‚¹é”™è¯¯ã€æ˜æ˜¾ä¸é€šé¡ºçš„è¯­ç—…ã€‚
2. ä¿æŒåŸä½œè€…çš„æ–‡é£å’Œç½‘æ–‡ç‰¹æœ‰çš„çˆ½æ„Ÿè¡¨è¾¾ï¼Œä¸è¦åšä¸å¿…è¦çš„æ¶¦è‰²å’Œè¿‡åº¦ä¿®æ”¹ã€‚

æ­£æ–‡å†…å®¹ï¼š
{content}

è¿”å›æ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
    "corrected_text": "å®Œæ•´çš„ä¿®æ­£åçš„æ­£æ–‡ï¼ˆå¿…é¡»å®Œæ•´åŒ…å«æ‰€æœ‰æ®µè½ï¼‰",
    "logs": ["å‘ç°[é”™åˆ«å­—/è¯­ç—…]ï¼šåŸå¥'...'ï¼Œä¿®æ”¹ä¸º'...'"]
}}
"""
        result = self._call_llm_json(sys_prompt, user_prompt)
        for log in result.get("logs", []):
            chap_name = self.meta["volumes"][v_idx]["chapters"][c_idx]["name"]
            self.log_signal.emit(f"âœï¸ [æ ¡å¯¹|{chap_name}] {log}")
        return result.get("corrected_text", content)

    def _do_setting_correction(self, v_idx, c_idx, content, summary, specific_reason=None):
        # ç»„è£…å…¨å±€å’Œå±€éƒ¨å¤§çº²ä½œä¸ºæ ‡å‡†
        global_synopsis = self.meta.get("global_synopsis", "")
        vol = self.meta["volumes"][v_idx]
        chap = vol["chapters"][c_idx]

        # ã€å‡çº§ç‚¹1ã€‘ï¼šè·å–è¿‡å¾€æ‰€æœ‰ç« èŠ‚çš„å‰§æƒ…æ¦‚è¦
        past_summaries = self._get_past_summaries(v_idx, c_idx)

        sys_prompt = "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ç½‘æ–‡ä¸»ç¼–ï¼Œç²¾é€šé€»è¾‘è‡ªæ´½å’Œè®¾å®šåœ†èã€‚å¿…é¡»è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ã€‚"

        # æ‹¼æ¥è±ªåç‰ˆä¸Šä¸‹æ–‡
        user_prompt = f"ã€å…¨ä¹¦æ€»ä½“è®¾å®šä¸æ¢—æ¦‚ã€‘ï¼š\n{global_synopsis}\n\n"
        if past_summaries.strip():
            user_prompt += f"ã€è¿‡å¾€å‰§æƒ…è½¨è¿¹(é˜²åƒä¹¦åŸºå‡†)ã€‘ï¼š\n{past_summaries}\n\n"
        user_prompt += f"ã€æœ¬å·æ ¸å¿ƒè®¾å®šã€‘ï¼š\n{vol.get('synopsis', 'æ— ')}\n\n"

        if specific_reason:
            # ã€å‡çº§ç‚¹2ã€‘ï¼šå…¨å±€çº é”™ä¼ å…¥äº†å…·ä½“ç†ç”±ï¼Œè¦æ±‚ç»“åˆå‰æ–‡è¯¦ç»†æ‰«æå¹¶ä¿®å¤
            user_prompt += f"ã€ç›®æ ‡ä»»åŠ¡ã€‘ï¼šè¿™æ˜¯å…¨å±€æ‰«æå‘ç°çš„æœ¬ç« é€»è¾‘/è®¾å®šé”™è¯¯ã€‚è¯·ç»“åˆä¸Šè¿°ã€å…¨ä¹¦è®¾å®šã€‘å’Œã€è¿‡å¾€å‰§æƒ…è½¨è¿¹ã€‘ï¼Œåœ¨ä¸‹æ–¹æ­£æ–‡ä¸­è¯¦ç»†æ‰«æå¹¶å½»åº•ä¿®å¤è¯¥é—®é¢˜ï¼š\n{specific_reason}\n\n"
        else:
            # å•ç« çº é”™æ¨¡å¼ï¼šè®© AI è‡ªå·±æ‰¾èŒ¬å¹¶ç»™å‡ºè¯¦ç»†ç†ç”±
            user_prompt += "ã€ç›®æ ‡ä»»åŠ¡ã€‘ï¼šè¯·ä»”ç»†æ¯”å¯¹ã€è¿‡å¾€å‰§æƒ…è½¨è¿¹ã€‘å’Œã€å…¨ä¹¦è®¾å®šã€‘ï¼Œæ£€æŸ¥ä¸‹æ–¹æ­£æ–‡ä¸­æ˜¯å¦å­˜åœ¨äººç‰©å´©å¡Œã€å‰è¨€ä¸æ­åè¯­ã€é€»è¾‘çŸ›ç›¾ï¼ˆåƒä¹¦ç°è±¡ï¼Œä¾‹å¦‚ï¼šæ­»äººå¤æ´»æœªè¯´æ˜åŸå› ã€ç‰©å“å½’å±é”™ä¹±ç­‰ï¼‰ã€‚è¯·å…ˆç»™å‡ºè¯¦ç»†çš„é”™è¯¯è¯Šæ–­ç†ç”±ï¼Œç„¶ååœ¨æ­£æ–‡ä¸­ç›´æ¥ä¿®å¤å®ƒä»¬ã€‚\n\n"

        user_prompt += f"ã€å½“å‰ç« èŠ‚æ­£æ–‡ã€‘ï¼š\n{content}\n\n"
        user_prompt += f"ã€å½“å‰ç« åŸAIæ¦‚è¦ã€‘ï¼š\n{summary}\n\n"

        # ã€å‡çº§ç‚¹3ã€‘ï¼šå¼ºåˆ¶è¦æ±‚è¾“å‡º error_reason å­—æ®µ
        user_prompt += """
è¿”å›æ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{
    "has_issue": true/false, // å¦‚æœæ²¡æœ‰å‘ç°ä»»ä½•é€»è¾‘è®¾å®šé”™è¯¯ï¼Œè¿”å›false
    "error_reason": "è¯¦ç»†çš„é”™è¯¯è¯Šæ–­ç†ç”±ã€‚å¦‚æœhas_issueä¸ºtrueï¼Œå¿…é¡»è¯´æ˜æ­£æ–‡å…·ä½“å“ªé‡Œåƒä¹¦æˆ–çŸ›ç›¾äº†ï¼Œä¸å‰æ–‡å“ªä¸€ç« å†²çªã€‚å¦‚æœä¸ºfalseåˆ™å¡«æ— ã€‚",
    "corrected_text": "ä¿®å¤åçš„å®Œæ•´æ­£æ–‡ï¼ˆå¦‚æœæ— é”™è¯¯ï¼ŒåŸæ ·è¿”å›ï¼‰",
    "new_ai_summary": "å¦‚æœæ­£æ–‡å‰§æƒ…è¢«ä¿®æ”¹ï¼Œè¯·åŒæ­¥æ›´æ–°AIæ¦‚è¦ï¼ˆçº¦500å­—ï¼Œå®¢è§‚çºªå®ç»“æ„åŒ–è®°å½•æ ¸å¿ƒäº‹ä»¶å’Œä¼ç¬”ï¼‰ã€‚å¦‚æœæ— ä¿®æ”¹åˆ™åŸæ ·è¿”å›ã€‚",
    "logs": ["å‘ç°[é€»è¾‘è®¾å®šé—®é¢˜]ï¼š...ï¼Œå› æ­¤ä¿®æ”¹äº†..."] // è®°å½•ç®€è¦çš„çº é”™åŠ¨ä½œ
}
"""
        result = self._call_llm_json(sys_prompt, user_prompt)

        if result.get("has_issue", False):
            # å°†è¯¦ç»†çš„è¯Šæ–­ç†ç”±æ‰“å°åˆ° UI çš„æ—¥å¿—ä¾§è¾¹æ ä¸­
            reason = result.get("error_reason", "")
            if reason and reason != "æ— ":
                self.log_signal.emit(f"ğŸ•µï¸ [è¯Šæ–­æŠ¥å‘Š|{chap['name']}] {reason}")

            for log in result.get("logs", []):
                self.log_signal.emit(f"ğŸ› ï¸ [è®¾å®šä¿®å¤|{chap['name']}] {log}")
            return result.get("corrected_text", content), result.get("new_ai_summary", summary)

        return content, summary

    def _detect_global_setting_conflicts(self):
        # æ‹¼æ¥å…¨ä¹¦æ¢—æ¦‚å’Œå·ç« ç”¨æˆ·è®¾çº²
        sys_context = f"ã€å…¨ä¹¦å…¨å±€å¤§çº²ã€‘\n{self.meta.get('global_synopsis', '')}\n\n"
        char_texts = [f"ã€{c['name']}ã€‘ æ€§åˆ«:{c['gender']} æ€§æ ¼:{c['personality']} ç»å†:{c['experience']}" for c in
                      self.meta.get("characters", [])]
        sys_context += f"ã€æ ¸å¿ƒäººç‰©è®¾å®šã€‘\n{chr(10).join(char_texts)}\n\n"

        # æ‹¼æ¥AIæ€»ç»“çš„æ‰€æœ‰ç« èŠ‚æ¦‚è¦
        all_summaries = ""
        for v_idx, vol in enumerate(self.meta["volumes"]):
            all_summaries += f"\nâ–¶ ç¬¬{v_idx + 1}å·: {vol['name']}\n"
            for c_idx, chap in enumerate(vol["chapters"]):
                all_summaries += f"  - ç¬¬{c_idx + 1}ç«  [{chap['name']}]: {chap.get('ai_synopsis', 'æš‚æ— æ¦‚è¦')}\n"

        sys_prompt = f"ä½ æ˜¯ä¸€ä¸ªç½‘æ–‡å‰§æƒ…è´¨æ£€ä¸“å®¶ã€‚è¿™æ˜¯æœ¬ä¹¦çš„æ ¸å¿ƒè®¾å®šåŸºçŸ³ï¼Œè¯·ç‰¢è®°ï¼š\n{sys_context}"
        user_prompt = f"""ä»¥ä¸‹æ˜¯AIæ€»ç»“çš„æœ¬ä¹¦ç›®å‰æ‰€æœ‰ç« èŠ‚çš„å‰§æƒ…æ¦‚è¦ã€‚
è¯·æ’æŸ¥æ˜¯å¦å­˜åœ¨ï¼š
1. æ˜æ˜¾åç¦»ã€å…¨å±€å¤§çº²ã€‘å’Œã€æ ¸å¿ƒäººç‰©è®¾å®šã€‘çš„å‰§æƒ…ã€‚
2. å†…éƒ¨é€»è¾‘çŸ›ç›¾ï¼ˆåƒä¹¦ç°è±¡ï¼Œä¾‹å¦‚ï¼šæ­»äººå¤æ´»æœªè¯´æ˜åŸå› ã€ç‰©å“å½’å±é”™ä¹±ã€äººç‰©æ€§æ ¼å˜åŒ–æå¤§ã€äººåä¸²å°ï¼‰ã€‚

æ¦‚è¦è®°å½•ï¼š
{all_summaries}

ä»»åŠ¡ï¼šå®šä½å­˜åœ¨ä¸¥é‡çŸ›ç›¾å’Œåƒä¹¦ç°è±¡çš„ç« èŠ‚ï¼Œå¹¶è¯¦ç»†è¯´æ˜é”™å› ã€‚
è¿”å›æ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
    "problematic_chapters": [
        {{
            "v_idx": å·ç´¢å¼•(æ•´æ•°ï¼Œä»0å¼€å§‹),
            "c_idx": ç« ç´¢å¼•(æ•´æ•°ï¼Œä»0å¼€å§‹),
            "reason": "è¯¦ç»†è¯´æ˜é”™åœ¨å“ªé‡Œï¼Œä¸å“ªä¸€éƒ¨åˆ†è®¾å®šæˆ–å‰é¢å“ªä¸€ç« äº§ç”Ÿäº†çŸ›ç›¾"
        }}
    ]
}}
å¦‚æœå®Œå…¨æ²¡æœ‰çŸ›ç›¾ï¼Œ"problematic_chapters"è¿”å›ç©ºæ•°ç»„ã€‚
"""
        result = self._call_llm_json(sys_prompt, user_prompt)
        return result.get("problematic_chapters", [])

    def _get_past_summaries(self, target_v_idx, target_c_idx):
        """è·å–ç›®æ ‡ç« èŠ‚ä¹‹å‰çš„æ‰€æœ‰å‰§æƒ…æ¦‚è¦ï¼ˆä½œä¸ºé˜²åƒä¹¦çš„è®°å¿†åŸºå‡†ï¼‰"""
        history_str = ""
        for v_idx in range(target_v_idx + 1):
            vol = self.meta["volumes"][v_idx]
            history_str += f"\nâ–¶ ç¬¬{v_idx + 1}å·: {vol['name']} (æœ¬å·æ¢—æ¦‚: {vol.get('synopsis', 'æ— ')})\n"

            # é™åˆ¶ç« èŠ‚éå†èŒƒå›´ï¼šå¦‚æœæ˜¯ç›®æ ‡ç« èŠ‚æ‰€åœ¨å·ï¼Œåªéå†åˆ°ç›®æ ‡ç« èŠ‚ä¹‹å‰ï¼›å¦‚æœæ˜¯ä¹‹å‰çš„å·ï¼Œéå†æ•´å·
            chap_limit = target_c_idx if v_idx == target_v_idx else len(vol["chapters"])
            for c_idx in range(chap_limit):
                chap = vol["chapters"][c_idx]
                # ä¼˜å…ˆè¯»å– AI ä¹‹å‰ç”Ÿæˆçš„è¯¦ç»†æ¢—æ¦‚ï¼Œæ²¡æœ‰åˆ™è¯»ç”¨æˆ·çš„
                ai_syn = chap.get("ai_synopsis", "")
                user_syn = chap.get("synopsis", "")
                display_syn = ai_syn if ai_syn.strip() else (user_syn if user_syn.strip() else "æš‚æ— æ¦‚è¦")

                history_str += f"  - ç¬¬{c_idx + 1}ç«  [{chap['name']}]: {display_syn}\n"
        return history_str